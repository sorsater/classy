Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Canicatti,
author = {Canicatti, Anthony},
file = {:home/michael/kurser/text-mining-tdde16/projekt/class.pdf:pdf},
isbn = {1601324316},
keywords = {and testing a classification,classification,genre,is necessary to begin,lyrics,model,song,text,training,with their corresponding genres},
pages = {44--49},
title = {{Song Genre Classification via Lyric Text Mining}}
}
@article{Tsaptsinos2017,
abstract = {Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure - in which words combine to form lines, lines form segments, and segments form a complete song - we adapt a hierarchical attention network (HAN) to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres.},
archivePrefix = {arXiv},
arxivId = {1707.04678},
author = {Tsaptsinos, Alexandros},
eprint = {1707.04678},
file = {:home/michael/kurser/text-mining-tdde16/projekt/HAN.pdf:pdf},
title = {{Lyrics-Based Music Genre Classification Using a Hierarchical Attention Network}},
url = {http://arxiv.org/abs/1707.04678},
year = {2017}
}
@article{Hu2009,
abstract = {This research examines the role lyric text can play in improving audio music mood classification. A new method is proposed to build a large ground truth set of 5,585 songs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relatively complete set of lyric features and representation models were investigated. The best performing lyric feature set was also compared to a leading audio-based system. In combining lyric and audio sources, hybrid feature sets built with three different feature selection methods were also examined. The results show patterns at odds with findings in previous studies: audio features do not always outperform lyrics features, and combining lyrics and audio features can improve performance in many mood categories, but not all of them.},
author = {Hu, Xiao and Downie, J. Stephen and Ehmann, Andreas F.},
file = {:home/michael/kurser/text-mining-tdde16/projekt/mood.pdf:pdf},
isbn = {9780981353708},
journal = {American Music},
number = {Ismir},
pages = {411--416},
title = {{Lyric text mining in music mood classification}},
volume = {183},
year = {2009}
}
@article{Tsaptsinos2017a,
abstract = {Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure - in which words combine to form lines, lines form segments, and segments form a complete song - we adapt a hierarchical attention network (HAN) to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres.},
archivePrefix = {arXiv},
arxivId = {1707.04678},
author = {Tsaptsinos, Alexandros},
eprint = {1707.04678},
file = {:home/michael/kurser/text-mining-tdde16/projekt/HAN2.pdf:pdf},
title = {{Lyrics-Based Music Genre Classification Using a Hierarchical Attention Network}},
url = {http://arxiv.org/abs/1707.04678},
year = {2017}
}

@Article{Einstein,
  author =       "Albert Einstein",
  title =        "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
                 [{On} the electrodynamics of moving bodies]",
  journal =      "Annalen der Physik",
  volume =       "322",
  number =       "10",
  pages =        "891--921",
  year =         "1905",
  DOI =          "http://dx.doi.org/10.1002/andp.19053221004"
} 

@book{Freud,
    author = "Freud, Sigmund",
    title = "The fucking classy interpretation of dreams",
    publisher = "Avon",
    year = "1900",
    note = "1965 ed.",
}


